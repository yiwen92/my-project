import re, json, math, os, logging, codecs, jieba
from collections import defaultdict
from tqdm import tqdm
from config import conf

re_ch = re.compile(u"([\u4e00-\u9fa5])",re.S)
re_en = re.compile(u"([a-zA-Z]+|[0-9]+k[\+]*)",re.S)
re_year = re.compile(u'([0-9]*å¹´)', re.M | re.I)
PUNCTUATION_LIST = ".ã€‚,ï¼Œ,ã€?ï¼Ÿ:ï¼š;ï¼›{}[]ã€ã€‘â€œâ€˜â€™â€ã€Šã€‹/!ï¼%â€¦â€¦ï¼ˆï¼‰<>@#$~^ï¿¥%&*\"\'=+-_â€”â€”ã€Œã€"
NONE_STOPWORD = []
CUSTOM_STOPWORD = ["äºº","å¹´","å¤§","å·¥ä½œ","èƒ½åŠ›","è´Ÿè´£","è´Ÿè´£","ç”Ÿäº§"]
STOP_WORDS = [e.strip() for e in open(conf.stop_words, encoding="utf8").readlines() if e.strip() not in NONE_STOPWORD] + CUSTOM_STOPWORD

def load_place(path):
    res = []
    txt = [e.strip().split(",")[-1] for e in open(path, encoding="utf8").readlines()[1:]]
    for w in txt:
        if w.endswith("å¸‚") or w.endswith("çœ"): res.append(w[:-1])
        res.append(w)
    return res

PLACE_NAMES = load_place(conf.place_names)

def contain_chinese_word(sentence):
    if re_ch.findall(sentence): return True
    return False

def invalid_entity(word):
    for e in PLACE_NAMES:
        if e in word and word != e: return True     # è¿‡æ»¤å®ä½“ä¸­åŒ…å«åœ°å€çš„è¯
    if re_year.findall(word): return True             # è¿‡æ»¤å®ä½“ä¸­åŒ…å«æ—¶é—´çš„è¯
    return False

def gen_entity_dict():
    for file_name in ['__func__.txt', '__ind__.txt']:
        res = []
        text = open('dict/' + file_name, encoding='utf8').readlines()
        sub_word = ""
        for i, line in enumerate(text):
            #line = "å‰ç«¯å¼€å‘1å¹´"
            cur_word = line.strip().replace(" ", "")
            if cur_word in ['å¼€å‘å·¥ç¨‹']: continue
            if sub_word and contain_chinese_word(cur_word) and sub_word in cur_word and cur_word.index(sub_word) == 0: continue
            elif invalid_entity(cur_word): continue
            else:
                res.append(cur_word + ' 10000\n')
                sub_word = cur_word
        with open('dict/' + file_name.replace("__", ""), "w", encoding="utf8") as fin:
            fin.write("".join(res))

def load_word_freq_dict(path, th=0):      # åŠ è½½è¯å…¸
    matchObj = re.compile(r'(.+) ([0-9]+)', re.M | re.I)
    word_freq = {}
    if not os.path.exists(path):
        logging.warning("file not exists:" + path)
        return word_freq
    with codecs.open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith('#'): continue
            matchRes = matchObj.match(line)
            word, freq = matchRes.group(1), int(matchRes.group(2))
            if freq < th: continue
            word_freq[word] = freq
    return word_freq

FUNC_DICT = load_word_freq_dict(conf.func_file)
INDUS_DICT = load_word_freq_dict(conf.indus_file)

def term_type(word_index, sen2terms):
    """
    0-ä¸­æ–‡: å‰ç«¯å¼€å‘ï¼Œ1-è‹±æ–‡ï¼šwebï¼Œ2-æ•°å­—ï¼š2000ï¼Œ3-ç¬¦å·ï¼škï¼Œ4-åœç”¨è¯ï¼šçš„ï¼Œ5-å…¶å®ƒï¼šapp123
    """
    type_encode = [0] * 6
    ty_dict = {'ch': 0, 'en': 1, 'digit': 2, 'punct': 3, 'stopword': 4, 'other': 5}
    def is_ch(w):
        if re_ch.findall(w): return True
        return False
    def is_digit(w):
        if w.isdigit(): return True
        return False
    def is_en(w):
        if w.isalpha(): return True
        return False
    if word_index < 0 or word_index >= len(sen2terms):
        type_encode[ty_dict['other']] = 1
        return type_encode
    term = sen2terms[word_index]
    if is_ch(term): type_encode[ty_dict['ch']] = 1
    elif is_en(term): type_encode[ty_dict['en']] = 1
    elif is_digit(term): type_encode[ty_dict['digit']] = 1
    elif term in PUNCTUATION_LIST: type_encode[ty_dict['punct']] = 1
    elif term in STOP_WORDS: type_encode[ty_dict['stopword']] = 1
    else: type_encode[ty_dict['other']] = 1
    return type_encode

def entity_type(word_index, sen2terms):
    """ 0-è¡Œä¸šè¯ï¼Œ1-èŒèƒ½è¯, 3-å…¶å®ƒ """
    entiey_encode = [0] * 3
    ty_dict = {'indus': 0, 'func': 1, 'other': 2}
    if word_index < 0 or word_index >= len(sen2terms):
        entiey_encode[ty_dict['other']] = 1
        return entiey_encode
    term = sen2terms[word_index]
    if term in INDUS_DICT: entiey_encode[ty_dict['indus']] = 1
    elif term in FUNC_DICT: entiey_encode[ty_dict['func']] = 1
    else: entiey_encode[ty_dict['other']] = 1
    return entiey_encode

def Q2B(uchar):     # å…¨è§’è½¬åŠè§’
    inside_code = ord(uchar)
    if inside_code == 0x3000:
        inside_code = 0x0020
    else:
        inside_code -= 0xfee0
    if inside_code < 0x0020 or inside_code > 0x7e:  # è½¬å®Œä¹‹åä¸æ˜¯åŠè§’å­—ç¬¦è¿”å›åŸæ¥çš„å­—ç¬¦
        return uchar
    return chr(inside_code)

def stringQ2B(ustring):     # æŠŠå­—ç¬¦ä¸²å…¨è§’è½¬åŠè§’
    return "".join([Q2B(uchar) for uchar in ustring])

def uniform(ustring):       # æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼Œå®Œæˆå…¨è§’è½¬åŠè§’ï¼Œå¤§å†™è½¬å°å†™çš„å·¥ä½œ
    return stringQ2B(ustring).lower()

def clean_line(line):
    line = uniform(line)
    line = re.sub("[&$ï¿¥ï½ï¿½|ï¼ ï¼Ÿï¼ï¼ï¼œï¼›!ï½œï½›ï¼¼ï¼½ï¼»ï¼ï¼ï¼‹ï¼Š*ï¼†ï¼…ï¼ƒï¼‚ï¼ï¬ğŸŒï¼ï¹’ï©…ï¤Šïƒ˜ï·ï®ïµ]{1,}|[.#-]{2,}|[+]{3,}|[0-9]*%", "", line)
    line = re.sub("[ã€ã€‘]{1,}", "", line)
    return line

if __name__ == "__main__":
    a = clean_line("ï¼ˆä¸€ï¼‰ã€ä»»èŒèµ„æ ¼ã€‘ï¼š1ã€å¤§ä¸“åŠä»¥ä¸Šå­¦å†")
    #filter_ids("get_jdcv_data/jdcvids", "get_jdcv_data/sampleids")
    #cal_ndcg([5,6,3,2,4,1,0], 6)    #[3,2,3,0,1,2,3,0]
    #gen_entity_dict()